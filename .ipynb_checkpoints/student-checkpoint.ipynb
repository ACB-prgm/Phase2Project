{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: Aaron Bastian\n",
    "* Student pace: self paced\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: Joe Comeaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This linear regression analysis of the King County housing dataset will serve to inform real estate construction companies in Washington looking to invest in building a new home or residential community. The goals are to discover which features of a home increase its sale value and to quantitatively define the relationship between those features and the price of a home.\n",
    "\n",
    "This dataset contains over 15k data points representing house sales in King County between May 2014 - May 2015. Each data point includes data on amenities, number of bedrooms/bathrooms, square footage, renovations, location, views, and sale price.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6g/3tz5tpjs4zj1cqs2xh_sq0m40000gn/T/ipykernel_59587/2147608447.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m \u001b[0;31m# visualizations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m \u001b[0;31m# linear regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "import pandas as pd # data analysis and cleaning\n",
    "import numpy as np # mathmatical functions + nan values\n",
    "import matplotlib.pyplot as plt # visualizations\n",
    "import seaborn as sns # visualizations\n",
    "sns.set(rc={'figure.figsize':(10,8)})\n",
    "import statsmodels.api as sm # linear regression\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARING AT THE DATA\n",
    "\n",
    "I'll import the dataset and convert the ordinal string variables to integers to allow for early correlation screening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/kc_house_data.csv\").dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.grade = data.grade.apply(lambda x: int(x.split()[0]))\n",
    "data.waterfront = data.waterfront.map({\"NO\": 0, \"YES\": 1}) # make waterfront a binary numerical variable for correlations\n",
    "\n",
    "conditons = {'Average' : 2, 'Very Good' : 4, 'Good' : 3, 'Poor' : 0, 'Fair': 1}\n",
    "data.condition = data.condition.map(conditons) # map condition to its numerical categories to do correlations.\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.price\n",
    "y.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As price is heavily skewed, I will remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier_IQR(df):\n",
    "    Q1=df.quantile(0.25)\n",
    "    Q3=df.quantile(0.75)\n",
    "    IQR=Q3-Q1\n",
    "    df_final=df[~((df<(Q1-1.5*IQR)) | (df>(Q3+1.5*IQR)))] # remove values that are > or < 1.5*IQR\n",
    "    return df_final\n",
    "\n",
    "y = remove_outlier_IQR(y)\n",
    "y.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = data.loc[y.index].corrwith(y)\n",
    "strong_corrs = corrs[abs(corrs) > 0.3] # filter weak correlations out\n",
    "strong_corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDENTIFYING INTERACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[strong_corrs.index].drop(\"price\", axis=1) # get the data associated with those columns and remove dependant variable\n",
    "\n",
    "test_df = pd.concat([y, X], axis=1)\n",
    "heatmap = sns.heatmap(test_df.corr(), vmin=-1, vmax=1, annot=True, cmap=\"viridis\")\n",
    "\n",
    "pd.plotting.scatter_matrix(test_df, figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a strong relationship between `sqft_above` and `sqft_living`.  This makes sens of course, as they include eachother in their calculations.  To avoid issues of colinearity, it makes sense to only include `sqft_living` then, as it has the strongest correlation with price.  It also seems that all of the predictors have a similarly skewed distribution as the price except for grade.  Grade is defined by The [King County Assessor website](https://info.kingcounty.gov/assessor/esales/Glossary.aspx?type=r#g) as corresponding to the quality of contruction and materials and thus the cost of construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=[\"sqft_above\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\"waterfront\", \"view\"]\n",
    "dummies = pd.get_dummies(data[categoricals], columns=categoricals ,prefix=categoricals, prefix_sep=\"_\" , dtype=np.int64) # create dummy variables to include categorical variables\n",
    "dummies.drop(columns=[\"waterfront_0\", \"view_NONE\"], inplace=True) # drop the no waterfront and NONE views to compare those with to those without\n",
    "\n",
    "X = pd.concat([X, dummies], axis=1).loc[y.index] # ensure that the target and predictors are aligned with loc[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a simple function to display relevant model metrics and test for assumptions of linear regression.\n",
    "def evaluate_results(res, target:str=\"target\") -> None:\n",
    "\n",
    "    # Tests for linearity\n",
    "    fig_pr, ax_pr = plt.subplots(figsize=(10, 10))\n",
    "    sm.graphics.plot_partregress_grid(res, fig=fig_pr)\n",
    "    rainbow = sm.stats.diagnostic.linear_rainbow(res) # linear rainbow test to confirm relationship is linear\n",
    "\n",
    "    # Tests for heteroskedasticity\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(y, res.resid, alpha = 0.3)\n",
    "    ax.set_xlabel(target)\n",
    "    ax.set_ylabel(\"residuals\")\n",
    "    ax.set_title(\"Heteroscedastic Residual Plot\")\n",
    "    GoldQuand = sm.stats.diagnostic.het_goldfeldquandt(res.resid, res.model.exog, alternative=\"two-sided\")\n",
    "\n",
    "    # Test for normal distribution of residuals\n",
    "    sm.graphics.qqplot(res.resid, dist=stats.norm, line='45', fit=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(res.summary())\n",
    "    print(f\"with a Rainbow Test pvalue of {round(rainbow[1], 3)}, we {'can' if rainbow[1] > 0.05 else 'cannot'} assume the relationship is linear\")\n",
    "    print(f\"with a Goldfeld-Quandt Test pvalue of {GoldQuand[1]}, we {'can' if GoldQuand[1] > 0.05 else 'cannot'} assume homoscedasticity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "evaluate_results(results, \"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "The baseline model performed decently well, statistically signifficantly explaining about 65% of the variance in sale price.\n",
    "\n",
    "All of the predictors were statistically significant:    \n",
    "For each additional bathroom, there is a -7263.03 change in price  \n",
    "For each additional sqft_living, there is a +89.07 change in price  \n",
    "For each additional sqft_living15, there is a +33.78 change in price  \n",
    "For each additional lat, there is a +598492.93 change in price  \n",
    "For each additional grade, there is a +59058.73 change in price   \n",
    "Houses on a waterfront have a 131,255.15 higher price tag compared to those not on a waterfront  \n",
    "Houses with a \"fair\" view have a 95,636.39 higher price tag than those with \"no view\"  \n",
    "Houses with a \"average\" view have a 90,037.47 higher price tag than those with \"no view\"  \n",
    "Houses with a \"good\" view have a 105,346.57 higher price tag than those with \"no view\"  \n",
    "Houses with a \"excellent\" view have a 173,596.66 higher price tag than those with \"no view\"  \n",
    "\n",
    "It is odd that an increase in bathrooms supposedly decreases the price, so I will investigate that.\n",
    "\n",
    "The assumptions of Linearity, Indepencance, Normality, Homoscedasticity (equal variances of the residuals/errors) hold in this model.  The Heterscedasticity plot and qq-plot look slightly irregular however, an as our target variable `price` had a slightly irregular distribution, I will log transform it to see if this improves the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2\n",
    "y_log = np.log(y)\n",
    "results_2 = sm.OLS(y_log, sm.add_constant(X)).fit()\n",
    "evaluate_results(results_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "The model remains significant with an increase in R2 from 0.648 to 0.668 (+0.02).  We can also see the qq-plot looks even more like a normal distribution, and our rainbow pvalue increased.  The Heterscedasticity plot looks slightly more Homoscedastic, but the Goldfeld-Quandt Test says otherwise.  The log transformation has also made the bathrooms coefficient positive. I do not want to log transform the predictors, so I will look at adding another predictor from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "new_data = data.loc[y.index]\n",
    "new_data.plot.scatter(x=\"sqft_living\", y=\"price\", c=\"floors\", cmap=\"plasma\", alpha=0.6, ax=ax) # check for interactions between sqft_living and other variables at c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for interaction terms, I stumbled upon this interaction between `sqft_living` and `floors`, where it seems that there are ~3 separate populations within floors.  I will add `floors` to the independant variables as dummies as it is a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floor_dums = pd.get_dummies(data.loc[y.index].floors, prefix=\"floors\", drop_first=True, dtype=np.int64)  # create dummies for floor variable\n",
    "X2 = pd.concat([X.copy(), floor_dums], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3\n",
    "results_3 = sm.OLS(y_log, sm.add_constant(X2)).fit()\n",
    "evaluate_results(results_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "This seems to have slightly improved the model.  The adj. R2 increased modestly from 0.668 to 0.677 (0.009), and reduced our Heteroscedasticity.  `floors_1.5` and `floors_2.5` seem to be statistically signifficant, while the others are not.\n",
    "\n",
    "Now I will see what happens when I create the interaction terms between `sqft_living` and `floors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4\n",
    "interactions = floor_dums.multiply(X2.sqft_living, axis=0) # create a df of interaction terms\n",
    "interactions.columns += \"_X_sqft_living\" # rename columns to reflect the modification\n",
    "\n",
    "X3 = pd.concat([X2.copy(), interactions], axis=1) # add to old independant vars\n",
    "\n",
    "results_4 = sm.OLS(y_log, sm.add_constant(X3)).fit()\n",
    "evaluate_results(results_4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "This did not seem to improve the model, as our adj R2 only increased by 0.001, our heteroscedasticity increased, and our model has become more complex to interpret.  Finally, I will try applying the natural log to `sqft_living` and `sqft_living15` as they seemed to have non-normal distributions and may have a non-linear relationship with `price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5\n",
    "X4 = X2.copy()\n",
    "X4.sqft_living = np.log(X4.sqft_living + 1e-5)\n",
    "X4.sqft_living15 = np.log(X4.sqft_living15 + 1e-5)\n",
    "# Log transform the sqft predictors\n",
    "\n",
    "results_5 = sm.OLS(y_log, sm.add_constant(X4)).fit()\n",
    "evaluate_results(results_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION\n",
    "\n",
    "I don't see many ways to improve the model much further.  I will select this fifth model to best describe this data.  The analysis follows:\n",
    "\n",
    "First, I will remodel the same data with the independent variables normalized in order to compare their effects on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = X4.copy()\n",
    "X_norm = (X_norm - X_norm.mean()) / X_norm.std() # Normalize the ind. variables by subtracting their means and dividing by their standard deviations\n",
    "res_norm = sm.OLS(y_log, sm.add_constant(X_norm)).fit() # create a new model with the normalized X\n",
    "\n",
    "to_include = res_norm.params[res_norm.pvalues < 0.05][1:].sort_values() # get the significant ind. variables coefficients and sort them \n",
    "fig, ax = plt.subplots(figsize=(5,6), dpi=100)\n",
    "ax.scatter(to_include, range(len(to_include)), color=\"#1a9988\", zorder=2)\n",
    "ax.set_yticks(range(len(to_include)), to_include.index)\n",
    "ax.set_xlabel(\"Proportional Effect\")\n",
    "ax.set_title(\"Strength of Relationships\")\n",
    "\n",
    "for idx, ci in enumerate(res_norm.conf_int().loc[to_include.index].iterrows()): # add hlines as error bars based upon the confidence intervals\n",
    "    ax.hlines(idx, ci[1][0], ci[1][1], color=\"#eb5600\", zorder=1, linewidth=3)\n",
    "\n",
    "plt.axline((0,0), (0,1), color=\"#eb5600\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "The model statistically signifficantly explains ~68% of the variance in sale price.\n",
    "\n",
    "From the normalized results, we can see that `sqft_living`, `grade`, and `lat` have the most effect on our model. Of those, sqft_living and grade are the most actionable for the purposes of a construction company.\n",
    "\n",
    "For each additional bathroom, we see an associated increase of ~1.31% in price.  \n",
    "**For each additional grade, we see an associated increase of ~13.35% in price.**  \n",
    "For each additional degree of latitude in King County, we see an associated increase of ~321.30% in price.  \n",
    "**For each increase of 1% sqft_living, we see an associated increase of ~0.32% in price.**  \n",
    "For each increase of 1% sqft_living15, we see an associated increase of ~0.17% in price.  \n",
    "Houses on a waterfront have a ~42% higher price compared to those not on a waterfront.  \n",
    "Houses with a \"fair\" view have a ~20.5% higher price tag than those with \"no view\".  \n",
    "Houses with a \"average\" view have a ~18.6% higher price tag than those with \"no view\".  \n",
    "Houses with a \"good\" view have a ~22.4% higher price tag than those with \"no view\".  \n",
    "Houses with a \"excellent\" view have a ~31.4% higher price tag than those with \"no view\".  \n",
    "Houses with 1.5 floors have a ~14.8% higher price tag than those with one floor.  \n",
    "Houses with 2.5 floors have a ~11.1% higher price tag than those with one floor.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion and Caveats\n",
    "Though we have a decent model, several of the predictors are not particularly useful.  Construction companies can rarely control whether their houses will be waterfront properties, have excellent views, or be further from the equator.  I considered removing latitude as it did not seem logical to me, but I left it in as I don't know enough about the geography of King County, Washington to say definitively that northernmost areas are considered more desireable.  For instance, maybe northernmost houses are closer to big cities, or are nearer to better school districts.  Grade, while a useful variable describing the quality of contruction and materials, is not speciffic enough to generate any real insight other than \"you get what you pay for\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
